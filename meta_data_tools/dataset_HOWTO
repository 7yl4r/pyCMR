This HOWTO starts with how to make the simplest dataset level python script and adds on features a step at a time.

MINIMAL

Start with a new work directory and change directory to it.
Create a new file called "new.py" with the contents:
----------
#!/usr/bin/python
import sys
sys.path.append("/usr/local/lib/granule-metadata-tools")
from print_metadata import print_metadata

# The root of where ds_url refers to.
# The 'path=' metadata for files will be relative to this directory
rootDir = "."

# The file patterns to look at and the functions to parse them followed by arguments for the parser
patternsToParsers = (
                      ("*", None),
                    )

print_metadata(rootDir, patternsToParsers)
----------
Now chmod +x on new.py, and run it:
[nwharton@infield work]$ ./new.py
file=new.py,path=new.py,size=473,checksum=9a625c01d27fd29bf29c24f174d9f031928ce7bd

This will confirm that the granule-metadata-tools is installed on your system.

The first 4 lines of the script are going to be the same for every metadata script.  They basically say this is a python script, I want to use the python module "sys" to append granule-metadata-tools to my module search path, and from that directory I want to use the print_metadata function from the file print_metadata.

The print_metadata function requires only 2 parameters to do its minimal functionality.  It requires a directory to use as the root of its search and a structure called patternsToParsers.  The structure patternsToParsers contains filename patterns associated with actions to take on files that match that pattern.  Files are matched against patterns in order, and the first pattern that matches determines the actions to be taken.

In our first example we just have a pattern that matches all files and set the actions to None.  This causes all files to have entries in the output metadata, but those files aren't run through any parsers.  This causes only the basic metadata to be printed (file, path, size, and checksum).

ADDING STATIC DATA

To add additional metadata fields that are constant throughout the dataset, create a hashtable containing that data and pass that as the third argument to print_metadata.  For example:
----------
#!/usr/bin/python
import sys
sys.path.append("/usr/local/lib/granule-metadata-tools")
from print_metadata import print_metadata
import socket

# The root of where ds_url refers to.
# The 'path=' metadata for files will be relative to this directory
rootDir = "."

# The file patterns to look at and the functions to parse them followed by arguments for the parser
patternsToParsers = (
                      ("*", None),
                    )

# The variables that are the same for every line
static_data = {}
static_data["host"] = socket.gethostname().split(".")[0]  # Only the part before the first '.'
static_data["env"] = "ops"
static_data["project"] = "IFLOODS"
static_data["ds"] = "gpmrgnaifld2"
static_data["inv"] = "GPM_INV"
static_data["browse"] = "N"

print_metadata(rootDir, patternsToParsers, static_data)
----------
Under "from print_metadata import print_metadata" we added "import socket" so we could use that module to get the hostname of the current machine to be used in the static data.  These 6 new fields are added to the basic 4 in the output, and now we get:
host=infield,env=ops,project=IFLOODS,ds=gpmrgnaifld2,inv=GPM_INV,file=new.py,path=new.py,size=818,browse=N,checksum=f2c2414a3d9dd538dd1dce9c44c09d9f69e75998

LIMITING TOP LEVEL DIRECTORIES

Many times the root directory contains several other directories that contains items for which we don't want metadata.  We can limit which directories to inspect by adding a list of them as the 4th argument to print_metadata.

Type "mkdir tables ; touch tables/table_1.txt"
Then make the print_metadata call look like:
print_metadata(rootDir, patternsToParsers, static_data, ("tables",))

The dangling comma after "tables" is important so that python knows this is a structure called a "tuple".  Otherwise it just looks like a string in parenthesis.  Tuples with more than one entry don't require a dangling comma.

Now the output looks like:
host=infield,env=ops,project=IFLOODS,ds=gpmrgnaifld2,inv=GPM_INV,file=table_1.txt,path=tables/table_1.txt,size=0,browse=N,checksum=da39a3ee5e6b4b0d3255bfef95601890afd80709

Notice that new.py is not being printed anymore, and path= looks different that file= now.

GETTING MORE DATA WITH PARSE FUNCTIONS

To add the min/max of time, lat, and lon, we are going to have to add a parser.  First, put some example data in tables/table_1.txt:
2013 IFloodS APU02 MetOne 0.254 mm
Year Mon Day Jday  Hr Min Rain[mm/h]  Lat        Lon
2013  04  10  100  02  22    8.31  42.18232  -92.36543
2013  04  11  100  02  33    8.31  43.18232  -91.36543
2013  04  12  100  02  44    7.31  44.18232  -90.36543

This is a table that has fields for year, month, day, hour, minute, lat, and lon.  There is a generic parser in granule-metadata-tools for this format called read_table_ymdhmll.  It needs zero based arguments to find the fields.  In this case, they are 0, 1, 2, 4, 5, 7, 8.

In general, before editing code, you can test if a parser is going to work on the command line by something like:
/usr/local/lib/granule-metadata-tools/read_table_ymdhmll.py tables/table_1.txt 0 1 2 4 5 7 8

If it prints out 6 values that look good:
end=2013-04-12 02:44:00
NLat=44.18232
SLat=42.18232
start=2013-04-10 02:22:00
WLon=-92.36543
ELon=-90.36543

You are ready to edit the dataset.py file.  It should now look like:
----------
#!/usr/bin/python
import sys
sys.path.append("/usr/local/lib/granule-metadata-tools")
from print_metadata import print_metadata
from read_table_ymdhmll import read_table_ymdhmll
import socket

# The root of where ds_url refers to.
# The 'path=' metadata for files will be relative to this directory
rootDir = "."

# The file patterns to look at and the functions to parse them followed by arguments for the parser
patternsToParsers = (
                      ("*.txt", (read_table_ymdhmll, 0, 1, 2, 4, 5, 7, 8)),
                      ("*", None),
                    )

# The variables that are the same for every line
static_data = {}
static_data["host"] = socket.gethostname().split(".")[0]  # Only the part before the first '.'
static_data["env"] = "ops"
static_data["project"] = "IFLOODS"
static_data["ds"] = "gpmrgnaifld2"
static_data["inv"] = "GPM_INV"
static_data["browse"] = "N"

print_metadata(rootDir, patternsToParsers, static_data, ("tables",))
----------
The 2 lines added were:
from read_table_ymdhmll import read_table_ymdhmll
                      ("*.txt", (read_table_ymdhmll, 0, 1, 2, 4, 5, 7, 8)),

Now when new.py is run, it prints:
host=infield,env=ops,project=IFLOODS,ds=gpmrgnaifld2,inv=GPM_INV,file=table_1.txt,path=tables/table_1.txt,size=253,start=2013-04-10T02:22:00Z,end=2013-04-12T02:44:00Z,browse=N,checksum=8edd33802c68228a1fdbc08d87734dddfad21c83,NLat=44.18232,SLat=42.18232,WLon=-92.36543,ELon=-90.36543

ls /usr/local/lib/granule-metadata-tools/read_*.py to get the current list of parsers.  If the documentation effort has progressed far enough, you should be able to do pydoc /usr/local/lib/granule-metadata-tools/read_*.py to browse the descriptions of the parsers and the required arguments.

POST PARSING

As additional feature requests arose, we found that we also needed to add a function to call after parsing was done.  This is so additional actions could be taken based on the metadata gathered after parsing the file.  The first feature was renaming a file based on the time period contained in it.

To add post parsing to a pattern, just add it after the parser and arguments in the patternsToParsers structure:
("*.txt", (read_table_ymdhmll, 0, 1, 2, 4, 5, 7, 8), renamer),

Then above the patternsToParsers (because it has to be defined before using it) define the renamer function.  It needs pathFilename and a hashtable as parameters.  pathFilename is relative to root, and the hashtable contains the metadata.

If the post parser returns a string, the file will have a symbolic link created to it from that string.  The scripts show_renaming and finish_renaming will show what was renamed and do the actual file moving.

Here is the example renamer function:
----------
import re
ymdFormat = "%Y%m%d"
def renamer(pathFilename, md):
    startDT = md["start"]
    endDT = md["end"]
    m = re.match(r"(.*/)table_(.*).txt", pathFilename)
    if m:
        return m.group(1) + "TABLE_" + m.group(2) + "_" + startDT.strftime(ymdFormat) + "_" + endDT.strftime(ymdFormat) + ".txt"
----------
First we have to import the re module to do regular expression matching.  Next is a format string used to format the datetime objects for use in the new filename.  Python uses strftime from the standard C library.  For more information on the format, do "man strftime".

The datetime objects in md (the metadata hashtable) are at the keys "start" and "end".  The first 2 lines of the function pull these out.

If the pathFilename matches the expression given, m will become a match return that has the regular expression groups filled out.  In this case, group 1 (.*/) is the path with the trailing slash.  Group 2 (.*) is everything between the _ and the .txt.  So the returned string is:
tables/TABLE_1_20130410_20130412.txt

After new.py runs, you can see the result of the renaming with:
[nwharton@infield work]$ /usr/local/lib/granule-metadata-tools/show_renaming.sh .
tables/TABLE_1_20130410_20130412.txt was tables/table_1.txt

or
[nwharton@infield work]$ ls -l tables/
total 4
lrwxrwxrwx 1 nwharton nwharton  38 Jul 22 12:28 TABLE_1_20130410_20130412.txt -> /home/nwharton/work/tables/table_1.txt
-rw-rw-r-- 1 nwharton nwharton 253 Jul 22 11:08 table_1.txt

Up until this point, renaming is not destructive.  To undo it you can just delete all the symbolic links.

To make the changes permanent, do:
-----------------------------
[nwharton@infield work]$ /usr/local/lib/granule-metadata-tools/finish_renaming.sh .
Are you sure you want to replace the symlinks under . with the files they point to ?
The files they point to will no longer be at their original location
y
Prune empty directories?
y
[nwharton@infield work]$ ls
new.py  tables
[nwharton@infield work]$ ls tables/
TABLE_1_20130410_20130412.txt
[nwharton@infield work]$ ls -l tables/
total 4
-rw-rw-r-- 1 nwharton nwharton 253 Jul 22 11:08 TABLE_1_20130410_20130412.txt
-----------------------------
The argument to finish_renaming.sh is the root directory of the dataset.
Prune empty directories will cause any directories vacated by all its files being moved elsewhere to be deleted.
The new filename has the same timestamp as the original.

Another thing you can do in the post parse function is just put in additional metadata based on pattern and not rename anything.  This is how you could set "format" based on the pattern.  So, a post parse function could just be:

def addFormat(pathFilename, md):
    md["format"] = "ASCII"

